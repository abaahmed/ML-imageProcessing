# -*- coding: utf-8 -*-
"""Ahmed_Assignment 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v3FBrN5Shm3nPD7cB3OtC5cLutlm9oZj

# Assignment 4

Turn in the assignment via Canvas.

To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)

Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).

Make sure you fill in any place that says "YOUR CODE HERE" or "YOUR ANSWER HERE", as well as your name below:
"""

NAME = "Abdullah Ahmed"
STUDENT_ID = "1655997"

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

import numpy as np
import seaborn as sns
import pandas as pd

"""## Data Exploration and Preprocessing

### Load the Fashion-MNIST dataset
Keras has lots of datsets that you can just load right into python numpy arrays, see: https://keras.io/datasets/

We will be using the Fashion-MNIST dataset, which is a cool little dataset with gray scale $28\times28$ images of articles of clothing.
Keep in mind that they will be downloaded from the internet, so it may take a while.
"""

fashion_mnist = keras.datasets.fashion_mnist
# splitting training and test data and corresponding labels 
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
class_dict = {i:class_name for i,class_name in enumerate(class_names)}

def show_image(index):
    plt.figure()
    # cmap=plt.cm.binary allows us to show the picture in grayscale
    plt.imshow(train_images[index], cmap=plt.cm.binary)
    plt.title(class_names[train_labels[index]])
    plt.colorbar() # adds a bar to the side with values
    plt.show()

show_image(0)

"""## Question 1: Data Preprocessing
As you can see above, the images are valued from $[0,255]$. This is the normal range for images. Recall from the previous lectures and excercises that we need to normalize our data.

In order to normalize our data to $[0,1]$ we use the equation:

$$x_{norm}=\frac{x-x_{min}}{x_{max}-x_{min}}$$

In our case we can assume that $x_{min}=0$ and $x_{max}=255$, this is a safe assumption since we are working with image data.

This means that for image data, if we want to normlize to $[0,1]$ the equation simplifies to:

$$img_{norm}=\frac{img}{255}$$

Anytime you work with image data in any kind of model you will be normalizing with this equation. Unless the range you want to normalize is different. Sometimes you want to normalize between $[-1,1]$, for that you would use a slightly different equation.

### Question 1.1) Normalizing the data
Normalize BOTH the training and testing images using the above equation.
"""

### YOUR CODE HERE ###
train_images = train_images / 255
test_images =  test_images / 255

"""If we show the image again, you will see the values are all scaled correctly."""

show_image(0)

# Lets sample our data to see what kind of images are stored.
# see documentation for subplot here:
# https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.subplot.html
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

"""### Question 1.2) Data Transformation
Since our data is composed of grayscale images (one channel) with a resolution of $28\times28$, we can think of this as the images existing in a $28\times28=784$ dimensional space. This means that every single image in our dataset can be represented by a vector of length $784$.

Please reshape BOTH the training and testing images to be $784D$. 

Hint: look into numpy.reshape().
"""

print(f'Before reshape, train_images shape: {train_images.shape} test_images shape: {test_images.shape}')
train_images = np.reshape(train_images, (train_images.shape[0], 784))
test_images = np.reshape(test_images, (test_images.shape[0], 784))
print(f'Before reshape, train_images shape: {train_images.shape} test_images shape: {test_images.shape}')

"""We create a dataframe using our training and testing data to keep everything tidy."""

# Add training data into a dataframe
img_data = {f"z{i}":train_images[:,i] for i in range(784)}
img_data["label"] = train_labels
df_img_train = pd.DataFrame(img_data)
df_img_train["class"] = df_img_train["label"].map(class_dict)
df_img_train.head()

# Add test data into a dataframe
img_data = {f"z{i}":test_images[:,i] for i in range(784)}
img_data["label"] = test_labels
df_img_test = pd.DataFrame(img_data)
df_img_test["class"] = df_img_test["label"].map(class_dict)
df_img_test.head()

"""Now we have our data reshaped into the $784D$ vectors, we can classify the images using a feed forward artifical neural network.

## Question 2: Neural Network
In this question we will build different neural network models. 

### Question 2.1) Pullovers vs. Coats. 

Build a neural network to your liking and train it for 50 epochs with a 0.2 train/validation split. You will want to use binary cross entropy ("bce") loss as the loss function. See if you can tune your model to achieve 70% accuracy.
"""

from tensorflow.keras.layers import Input, Dense # only use these layers
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import * # you can use any optimizer

# DEFINE YOUR MODEL HERE
input_layer = Input(shape = (784,))

x = Dense(100, activation='tanh')(input_layer)
output = Dense(1, activation='sigmoid')(x)
model = Model(input_layer, output)

# Show a summary of your model
model.summary()

# A function for getting a subset of the data
def get_data_subset(df, classes=[], shuffle=True, shuffle_seed=42):
    """
    Used to retrieve columns from df
    """
    if classes == []:
        print("Pleas")
    else:
        df_filtered = df[(df["class"] == classes[0]) | (df["class"] == classes[1])].copy()
        df_filtered["binary_label"] = 0
        df_filtered.loc[df["class"] == classes[1], "binary_label"] = 1
        data = df_filtered.filter(regex=("z[0-9]+")).values
        labels = df_filtered["binary_label"].values
        if shuffle:
            np.random.seed(shuffle_seed)
            np.random.shuffle(data)
            np.random.seed(shuffle_seed)
            np.random.shuffle(labels)

    return data, labels.reshape(-1,1)

# Preparing data for training, use get_data_subset along with df_img_train

X, y = get_data_subset(df_img_train, classes=["Pullover", "Coat"])

# Compile your model with your chosen optimizer, binary cross entropy for the loss, and accuracy as the metric

model.compile(loss='binary_crossentropy',
              optimizer=SGD(),
              metrics=['accuracy'])

# Call fit on your model passing in the train_images, train_labels data above, train for 50 epochs 0.2 train/validation split
hist =  model.fit(X, y, validation_split = 0.2,  epochs = 50)

def plot_losses(hist):
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()
def plot_accuracies(hist):
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()

# Plot your losses and accuracies
plot_losses(hist)
plot_accuracies(hist)

"""### Question 2.2) Observation
How did your neural network perform? What hyperparameters and optimizer did you choose?

My Neural Network performed well with training accuracy ~= 0.90, and validation accuracy ~= 0.88. I utilized a tanh activation for my hidden layer with a sigmoid output layer activation (better than softmax for binary classification).The optimizer I originally used was SGD (Stochastic Gradient Descent) with learning rate of 0.01, but after conducting some research and testing/tuning hyperparamaters, I thought the better-accuracy optimizer for my setup was Adam. However, Adam was contributing to overfitting on the training data, so I stuck with SGD. My hyperparameters were a validation size of 0.2 and epoch = 50. One reason the NN performed so much better than 70% accuracy was because the neural network is capable of classifying higher level features by using hidden layers. Each hidden layer can be used to combine several features together to make more complex features that differentiate between the coat and pullover with high accuracy. Lastly, using Binary Cross Entropy as the loss function was a no brainer, as its widely used to classify between 2 classes, as we're doing in this case.

### Question 2.3) Multi-class Neural Network

Next, we will build a neural network toimplement a multi-class classification. Remember that your input size is (784,) and the output layer needs 10 nodes (also remember that there is a certain activation function that needs to be encorporated with the output layer). See if your model can get about 70% classification accuracy.

**Hint: When compiling your model use loss=tf.keras.losses.[SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)
"""

from tensorflow.keras.layers import Input, Dense # only use these layers
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import * # you can use any optimizer
from tensorflow.keras import regularizers


# DEFINE YOUR MODEL HERE
input_layer = Input(shape=(784))
x = Dense(45, activation='tanh')(input_layer)
x = Dense(25, activation='tanh')(x)
out = Dense(10, activation='softmax', activity_regularizer=regularizers.l2(1e-5))(x)
   

model = Model(input_layer, out)

# Show a summary of your model
model.summary()

# Compile your model with your chosen optimizer, binary cross entropy for the loss, and accuracy as the metric
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=SGD(),
              metrics=['accuracy'])

# Call fit on your model passing in the train_images, train_labels data above with validation split of 0.2 and train for 100 epochs

hist =  model.fit(X, y, validation_split = 0.2,  epochs = 100)

def plot_losses(hist):
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()
def plot_accuracies(hist):
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'])
    plt.show()

# Plot your losses and accuracies
plot_losses(hist)
plot_accuracies(hist)

"""### Question 2.4) Observation
How did your neural network perform? What hyperparameters and optimizer did you choose?

The Neural Network performed well with a training accuracy of ~0.93 and validation accuracy of ~0.89. I used a couple hidden layers with tanh activations and a Softmax for the output layer, as softmax is much better suited for multi-class data. Additionally, I used SGD as the optimizer with a laerning rate of 0.001 and categorical cross entropy for the loss, as with categorical cross entropy you're not limited to how many classes your model can classify, so in this example it is a better fit. I also set epoch = 100 and validation size to 0.2. Now, I noticed significant overfitting on the validation set, so I used activity_regularizer on the softmax function, and this regularization helped to reduce the overfitting by some amount, also helping the overall accuracy.

## Question 3: Building a Convolutional Neural Network

In this exercise, we will build a classifier model that is able to distinguish between 10 different classes of images - airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We will follow these steps:

1. Explore the example data
2. Build a small convnet to solve our classification problem
3. Evaluate training and validation accuracy

###  Data Exploration and Preparation

We'll start by downloading the CIFAR-10 dataset from Keras.

This is a link to the dataset documentation:
https://keras.io/datasets/#cifar10-small-image-classification

And a link to the dataset source:
https://www.cs.toronto.edu/~kriz/cifar.html

Be sure to set your Runtime environment to include a GPU, as it will speed up the training considerably (this time that's important!).

#### Loading data into local variables
"""

from keras.datasets import cifar10

# Fetch the data:
(X, y), (_, _) = cifar10.load_data()

"""####  Import needed functions and libraries"""

# Commented out IPython magic to ensure Python compatibility.
# Ignore the warnings - Otherwise, TensorFlow tends to innundate one with far too many warnings.
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

# For matrix operations and dataframes.
import numpy as np

# Data visualizaton.
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
import random as rn
 
# Configure some defaults.
# %matplotlib inline  
style.use('fivethirtyeight')
sns.set(style='whitegrid',color_codes=True)

# Useful deep learning functions.
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop 
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dropout, Flatten, Activation
from tensorflow.keras.layers import Conv2D, MaxPooling2D

# Powerful deep learning module.
import tensorflow as tf

# For dealing with data.
import numpy as np

"""#### Data Preparation & Exploration
Let's take a look at a few of these images. Rerun this cell multiple times to see different images for each class.

You may notice that these images look low fidelity, which is because they are! As we increase our image size, we also increase our model complexity. What's important is that our classes are still distinguishable from each other.
"""

fig, ax = plt.subplots(2, 5)
fig.set_size_inches(10, 6)

for i in range(2):
    for j in range(5):
        c = j + 5*i # Class counter
        l = np.random.choice(np.where(y == c)[0], 1)[0] # Get a random image from class c
        ax[i, j].imshow(X[l])
        ax[i, j].set_title('Class: ' + str(y[l]))
        # Hide grid lines
        ax[i, j].grid(False)
        # Hide axes ticks
        ax[i, j].set_xticks([])
        ax[i, j].set_yticks([])
        
plt.tight_layout()

"""#### Let's take a look at the format of our data"""

print('X (images)', X.shape)
print('y (classes)', y.shape)

"""We can see that we have 50,000 samples, where each images is 32 by 32 pixels with 3 color channels: RGB.

For each of these images, we have a single label for which class they each belong to.

#### One-hot encode the labels, and normalize the data

Similarly to previous exercises, we want to one hot encode our class labels. We also want to normalize our image data similarly to the previous question.
"""

# One-hot encode those integer values of class labels
y = to_categorical(y, 10)


# Normalize all entries to the interval [0, 1]
X = X / 255.

"""### Question 3.1)
Create your own deep learning architecture, and train it on the dataset above. If you're unsure where to start, begin by referencing the in class exercises. 

One suggestion is to add several convolution layers each followed by a maxpooling layer. Towards the end you can add one or more fully connected layers. Dropout layers are often useful after each fully connected layer for overfitting, and you can try experimenting with that parameter. Your model should be able to reach **70% validation accuracy**.

You are responsible for your model architecture, hyperparameters, and optimizer. 

**HOWEVER, you are limited to a maximum of 50 epochs and 500,000 model parameters! You will lose points for exceeding these limits.**
"""

# This is where we define the architecture of our deep neural network.
model = Sequential()

model.add(Conv2D(filters= 25, kernel_size= (3, 3), padding= "same", activation= "relu", input_shape= (32, 32, 3)))
model.add(Conv2D(filters= 50, kernel_size= (3, 3), padding= "same", activation= "relu"))
model.add(Conv2D(filters= 100, kernel_size= (3, 3), padding= "same", activation= "relu"))

model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Conv2D(filters= 25, kernel_size= (3, 3), activation= "relu"))
model.add(Conv2D(filters= 50, kernel_size= (3, 3), activation= "relu"))

model.add(Conv2D(filters= 100, kernel_size= (3, 3), activation= "relu"))

model.add(MaxPooling2D(pool_size = (2, 2)))


model.add(Flatten())

model.add(Dense(128, activation = "relu"))
model.add(Dense(64, activation = "relu"))

model.add(Dropout(0.8))

model.add(Dense(10, activation = "softmax",))

# A batch is the size of each training chunk. We're implementing batch gradient descent, which is in between
# stochastic gradient descent and full gradient descent.
batchsize = 500

# Each epoch goes through the entire training set once
epochs = 47

opt = RMSprop(lr=0.001)

model.compile(optimizer = opt,
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

model.summary()
# MAXIMUM OF 500,000 PARAMETERS!

history = model.fit(X, 
                    y,
                    batch_size = batchsize,
                    epochs = epochs, 
                    validation_split = 0.2, # DON'T CHANGE validation_split!
                    verbose = 1)

"""### Question 3.2)
Create training and validation loss and accuracy plots as above.
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

